\documentclass[10pt,ignorenonframetext]{beamer}


<<include=FALSE,cache=FALSE>>=
require(knitr)
opts_chunk$set(tidy=FALSE,echo=FALSE,results='markup',fig.path='figs/fig',cache=FALSE,highlight=TRUE,width.cutoff=132,size='footnotesize',out.width='1.2\\textwidth',message=FALSE,comment=NA)
@

\usepackage{bowers-beamer}
\graphicspath{{.}{../images/}}
\newcommand{\framen}[1]{\frame{#1 \note{ }}}

\title[What is a $p$-value?]{What does it mean to test a hypothesis?}

\author[Bowers]{
  %Jake Bowers\inst{1}$,$\inst{2}
  Jake Bowers\inst{1}
}

\date[27 Feb 2018]{EGAP Learning Days 9: Uruguay}

\institute[Illinois]{
  Political Science \& Statistics \& NCSA @ University of Illinois

  \smallskip

  {jwbowers@illinois.edu   --- \href{http://jakebowers.org}{http://jakebowers.org} }
}

\renewcommand{\bibsection}{\subsection*{References}}


\begin{document}

\begin{frame}[plain,label=intro,noframenumbering]
  \titlepage
\end{frame}

\begin{frame}
	\frametitle{Key Points}

	Statistical inference (e.g. Hypothesis tests and confidence
	intervals) is \textbf{inference} --- reasoning about the unobserved.

	\medskip

	 $p$-values require probability distributions.

	 \medskip

	 \textcolor{blue}{Randomization} (or Design) +
	 \textcolor{orange}{a Hypothesis} + \textcolor{green}{a Test Statistic
	 Function} can provide
	 probability distributions representing the hypothesis (and thus
	 $p$-values).

\end{frame}

\section{Review: Hypotheses and Counterfactual Causal Inference}

\begin{frame}
	\frametitle{Counterfactual Causal \textcolor{orange}{Inference}}

	How can we use what we \textbf{see} to learn about
	\only<1>{what we want to \textbf{know}} \only<3>{$\text{a causal effect}_i=f(y_{i,1},y_{i,0})$}
	\only<2>{\textbf{\textcolor{blue}{potential outcomes}}}?


<<readdata, echo=FALSE>>=
newsdf <- read.csv("../Data/news.csv")
@

<<setupnewsdesigntab,echo=FALSE>>=
newsdf$y1 <- ifelse(newsdf$z==1,newsdf$r,"?")
newsdf$y0 <- ifelse(newsdf$z==0,newsdf$r,"?")

library(xtable)

newspapers.xtab<-xtable(newsdf[,c("city","s","z","rpre","r","Newspaper","y1","y0")],
			label="tab:newspapers",
			caption="Design and outcomes in the Newspapers Experiment.
			The Treatment column shows treatment randomized
			within pair with the
			newspaper ads as 1 and lack of treatment as 0. The
			potential outcomes are $y_{1}$ for treatment and
			$y_{0}$ for control.  \\citet{pana2006}
			provides more detail on the design of the experiment.",
			align=c("l","l","c","c","c","c","c","c","c"))
@

<<newsdesigntab,results='asis',echo=FALSE>>=
print(newspapers.xtab,
      sanitize.text.function=function(x){x},include.rownames=FALSE,include.colnames=FALSE,
      table.placement="!ht",##size="small",
      add.to.row=list(pos=list(-1),
		      command="&&& \\multicolumn{2}{c}{Turnout} \\\\ City &
		      Pair & Treatment & \\multicolumn{1}{c}{Baseline} &
		      \\multicolumn{1}{c}{Outcome} &
		      \\multicolumn{1}{c}{Newspaper} &
		      \\multicolumn{1}{c}{$y_{1}$} &
		      \\multicolumn{1}{c}{$y_{0}$}\\\\"),
      hline.after=c(0,nrow(newspapers.xtab)))
@

\only<3>{\textbf{\emph{Small group exercise:}} \textbf{Can you think of
		\textcolor{orange}{more than one way} to learn about the
		counterfactual causal effect of treatment using what we
		observe?}}

\end{frame}

Definition of a p-value. What does it mean?

\begin{frame}
  \frametitle{What is the true effect of the treatment assignment?}

  \only<1>{ \includegraphics[width=.8\textwidth]{cartoonNeymanBayesFisherCropped.pdf} }
  \only<2>{ \includegraphics[width=.8\textwidth]{cartoon3ATENeyman.pdf} }
  \only<3>{ \includegraphics[width=.8\textwidth]{cartoonBayes.pdf} }
  \only<4>{ \includegraphics[width=.8\textwidth]{cartoon4Fisher.pdf} }

\vfill

%%   \only<5->{See also \citet{pearl:2000a} and also
%% 	  \cite{richardson2013single}. For more on the potential outcomes
%% 	  approach see \cite{imbens2015causal}. }

\end{frame}


\section{Testing Fishers Sharp Null Hypothesis of No Effects: Where do
$p$-values come from?}



\begin{frame}[containsverbatim,allowframebreaks,shrink]
  \frametitle{Testing the Sharp Null of No Effects Practically}
  \framesubtitle{Ingredients: Hypothesis, Test Statistic Function, Distribution}

<<>>=
options(width=132)
library(randomizr)
@

<<sharpnulltest1,echo=TRUE,results='markup',highlight=TRUE,tidy=FALSE,tidy.opts=list(width.cutoff=130)>>=
newexperiment <- function(block){
	block_ra(blocks=block)
}

testStat1 <- function(y,z,b){
	## Diff of means
  ## y is outcome
  ## z is treatment (binary here)
  ## b is block
  if(length(unique(b))==1){
    ## if only one block, then this is a simple mean difference
    return(mean(y[z==1]) - mean(y[z==0]))
  } else {
    z <- z[order(b)]
    ys <- y[order(b)]
    ysdiffs <- ys[z==1] - ys[z==0]  ## assuming sorted by pair
    meandiffs <- mean(ysdiffs)
    return(meandiffs)
  }
}


testStat2 <- function(y,z,b){
	## Diff of mean ranks
  ## y is outcome
  ## z is treatment (binary here)
  ## b is block
  y <- rank(y) ##
  if(length(unique(b))==1){
    ## if only one block, then this is a simple mean difference
    return(mean(y[z==1]) - mean(y[z==0]))
  } else {
    z <- z[order(b)]
    ys <- y[order(b)]
    ysdiffs <- ys[z==1] - ys[z==0]  ## assuming sorted by pair
    meandiffs <- mean(ysdiffs)
    return(meandiffs)
  }
}
@

\end{frame}


\begin{frame}[containsverbatim,allowframebreaks]
  \frametitle{Testing the Sharp Null of No Effects Practically}
  \framesubtitle{Ingredients: Hypothesis, Test Statistic Function, Distribution}

  The probability distribution implied by the hypothesis is generated by
  repeating the experiment.

<<sharpnulltest1ps,echo=TRUE,results='markup',highlight=TRUE,tidy=FALSE,tidy.opts=list(width.cutoff=130)>>=
obsTestStat1 <- with(newsdf,testStat1(y=r,z=z,b=sF))
obsTestStat2 <- with(newsdf,testStat2(y=r,z=z,b=sF))
set.seed(12345)
refDistNull1 <- replicate(1000,with(newsdf,testStat1(y=r,z=newexperiment(b=sF),b=sF)))
refDistNull2 <- replicate(1000,with(newsdf,testStat2(y=r,z=newexperiment(b=sF),b=sF)))

p1SideMean <- mean(refDistNull1>=obsTestStat1)
p1SideRank<-mean(refDistNull2>=obsTestStat2)
p2SideMean<-2*min(mean(refDistNull1>=obsTestStat1),mean(refDistNull1<=obsTestStat1))
p2SideRank <- 2*min(mean(refDistNull2>=obsTestStat2),mean(refDistNull2<=obsTestStat2))
p2SideRank
p2SideMean
@

\end{frame}


\begin{frame}
	\frametitle{}

	Hypotheses+Design imply difference observations (identity...)

\end{frame}


\begin{frame}[containsverbatim]
	\frametitle{Mean Difference Test Statistic}

	 $H_0: y_{i1}=y_{i0}$, One-sided $p=6/16=.375$.

\centering
\includegraphics[width=.8\textwidth]{newspapersRandDist1.pdf}

<<echo=TRUE>>=
6/16
p1SideMean
@


\end{frame}



\begin{frame}[containsverbatim]
	\frametitle{Different Test Statistics Summarize Information
	Differently}

	- Power differences. Sensitivity to different alternatives.

  \centering
<<echo=FALSE, out.width=".6\\textwidth">>=
plot(density(refDistNull2),xlim=range(c(refDistNull1,refDistNull2)))
rug(jitter(refDistNull2),line= -1,lwd=2)
lines(density(refDistNull1),lty=2)
rug(jitter(refDistNull1),line=.5,lwd=2)
abline(v=obsTestStat2)
abline(v=obsTestStat1)
@

\end{frame}

\end{frame}


% \begin{frame}
%   \frametitle{The Newspapers Study}
%
%
% <<newsdesigntab2,results='asis',echo=FALSE>>=
% print(newspapers.xtab,
%       sanitize.text.function=function(x){x},include.rownames=FALSE,include.colnames=FALSE,
%       table.placement="!ht",##size="small",
%       add.to.row=list(pos=list(-1),
% 		      command="&&& \\multicolumn{2}{c}{Turnout} \\\\ City &
% 		      Pair & Treatment & \\multicolumn{1}{c}{Baseline} &
% 		      \\multicolumn{1}{c}{Outcome} &
% 		      \\multicolumn{1}{c}{Newspaper} &
% 		      \\multicolumn{1}{c}{$y_{1}$} &
% 		      \\multicolumn{1}{c}{$y_{0}$}\\\\"),
%       hline.after=c(0,nrow(newspapers.xtab)))
% @
%
% \end{frame}


\section{Testing Sharp Hypotheses No Effects and No Interference}

\begin{frame}
  \frametitle{Statistical inference for counterfactual quantities with interference?}

  \centering
  \includegraphics{complete-graph.pdf}

  \only<1>{

    \includegraphics[width=.95\textwidth]{interference-example.pdf}

  }

  \only<2>{
    \includegraphics[width=.95\textwidth]{interference-example-2.pdf}

    Introducing the \textbf{uniformity trial} $\equiv \by_{i,0000}$
    (Rosenbaum, 2007).
  }


\end{frame}


\section{Aspects of the approach: The tests do not require asymptopia.}

\begin{frame}[containsverbatim,shrink]
	\frametitle{Permutations vs Central Limit Theorem?}

	Which distribution should we use? Which $p$-value is better? (On what
	basis would we judge a $p$-value?)

<<lmps,echo=TRUE,results='markup',highlight=TRUE,tidy=FALSE,tidy.opts=list(width.cutoff=130)>>=
lmpvalue <- summary(lm(r~z+sF,data=newsdf))$coef["z",4]
lmpvalue
p2SideMean
@

(1) False positive rate

(2) Power

\end{frame}


\begin{frame}
	\frametitle{Permutations vs Central Limit Theorem?: False Positive
	Rate}

\centering
\only<1>{\includegraphics[width=.95\textwidth]{newspapersTDist.pdf} }
\only<2>{\includegraphics[width=.95\textwidth]{newspapersTypeIError.pdf} }

\end{frame}


\begin{frame}[containsverbatim,shrink]
	\frametitle{What about when samples are large?}

	\begin{columns}[t]
		\begin{column}{.5\textwidth}
<<largeN, echo=TRUE>>=
set.seed(20180225)
N <- 1000
y0 <- rchisq(N,df=1)
y1 <- y0 + rnorm(N,mean=0,sd=sd(y0))## No treatment effect but different variance
Z <- complete_ra(N) ## very simple randomization
Y <- Z*y1 + (1-Z) * y0 ## randomization reveals a potential outcome

newexpComplete <- function(N){
	complete_ra(N)
}

testStat3 <- function(y,z){
	## Diff of means
  ## y is outcome
  ## z is treatment (binary here)
    md <- mean(y[z==1]) - mean(y[z==0])
    return(md)
  }

obsTestStat <- testStat3(y=Y,z=Z)
randDist <- replicate(1000,testStat3(y=Y,z=newexpComplete(N)))
mean(randDist >= obsTestStat)
2* min( mean(randDist >= obsTestStat), mean(randDist <= obsTestStat))
@

<<>>=
library(sandwich)
library(lmtest)

thelm <- lm(Y~Z)
theSEs <- coeftest(thelm,vcov=vcovHC(thelm,type="HC2"))
theSEs[2,4]
@
\end{column}
\begin{column}{.5\textwidth}

Notice heteroskedasticity.

<<>>=
 boxplot(list(y0,y1))
@


<<>>=
plot(density(randDist))
curve(dnorm(x,sd=theSEs[2,2],mean=0),from=-1,to=1,col="blue", add=TRUE)
@

\end{column}
\end{columns}

\end{frame}

\section{What is a $p$-value? What does a $p$-value require?}

\begin{frame}
	\frametitle{What does it mean to test a hypothesis?}

	What is a $p$-value?

	\medskip

	What do we learn from a hypothesis test?

	\medskip

	What do we need to assume to make a hypothesis test (in this simplest
	case) interpretable?

\end{frame}

\appendix
\section{Summary and Discussion}


\begin{frame}
\frametitle{Why test a hypothesis? What is a hypothesis?}

(1) In an experiment a hypothesis is a claim or model about the relationship among potential
outcomes; about unobserved outcomes.

(2) In an experiment a hypothesis test allows a form of causal inference (or statistical
inference about a counterfactual causal effect)

\end{frame}

\begin{frame}
	\frametitle{Where does the reference distribution come from? What
	justifies it?}

How to make a distribution? Where do they come from? How do we justify our
choices?

In an experiment we have the opportunity to use randomization to provide a
"reasoned basis for inference". That is, we know how randomization happened.
And the specific randomization could have happened differently (different
units could have been in treatment and control depending on the random process).




\end{frame}

\begin{frame}
	\frametitle{Key features of Fisher's approach}

	\begin{description}
		\item[Flexible] Any scientific model than can generate
			implications for all units' potential outcomes can, in
			principle, produce testable parameters.
		\item[Design based] Requires knowledge of probability of $Z$
			not $Y$ or $Y|X$ or $\beta|\gamma$.
		\item[Finite Sample Oriented] Does not require asymptopia. Can
			use asymptopia when there for a visit.
		\item[Can be slow] In between 8 cities and asymptopia is a
			land of many permutations.
		\item[Probably conservative] Uses relatively little of the
			total information we have available about the science.
	\end{description}

	\textcolor{orange}{If you want to know more read Paul Rosenbaum's work}
	The version of Fisher's approach I discuss here is built on work by Paul
	Rosenbaum. Read his work if you want to learn more.

\end{frame}



\begin{frame}[allowframebreaks]
        \bibliographystyle{apsr}
        \bibliography{../BIB/big}
\end{frame}

\section{Extra}

\subsection{Sharp Hypotheses of Some Effects. Sharp Hypotheses as Causal Models}

\subsubsection{The Constant Additive Effect Model}

\begin{frame}
	\frametitle{What range of effects might be surprising?}

	\begin{description}
		\item[Hypothesize a model of potential outcomes]
	For $H_0: y_{i1} = y_{i0} + \tau$, what $\tau$ might be surprising?

\item[Map the model to observation via design] What would $\tau=6$ imply for
	what we observe? If $\tau=6$ and $Y_i = Z_i y_{i1} + (1-Z_i) y_{i0}$
	then $Y_i - Z_i 6 = y_{i0}$.

\item[Generate the randomization distribution of this hypothesis] As before
\item[Summarize information against the hypothesis] For hypotheses of $\tau
	\ge 6$ we have $p \le .125$  using a mean difference test
	statistic.
\end{description}

\centering
\includegraphics[width=.7\textwidth]{newspapersFig3.pdf}

\end{frame}

\begin{frame}
	\frametitle{What effects might be least surprising?}

	The idea of a ``best guess'' maps onto the Hodges-Lehmann point
	estimate: the hypotheses for which $E(t(Z,Y)=0$: ex. the difference of
	means is zero. Here $\tau=1.5$ for the mean difference and
	$\tau=3.25$ for the rank-based test (which equalizes medians).

\centering
\includegraphics[width=.9\textwidth]{newspapersHLAdj.pdf}

\end{frame}

\begin{frame}
	\frametitle{Theoretical models of potential outcomes can produce sharp
	hypotheses}


\centering
\only<1>{ \includegraphics[width=.9\textwidth]{fishersutvaNetwork.pdf} }
\only<2>{ \includegraphics[width=.9\textwidth]{fishersutvaModel.pdf} }
\only<3>{ \includegraphics[width=.9\textwidth]{fishersutvaResults.pdf} }



\end{frame}

\begin{frame}{A General Fisherian Inference Algorithm}
  \begin{enumerate}
    \item Write a model ($\HH(\yu, \bz, \theta)$) converting uniformity trial
      into observed data (i.e. a causal model).
    \item Solve for $\yu$: $\HH(\by_\bz, \bzero, \theta_0) = \yu$
    \item Select a test statistic that is effect increasing in all relevant
	    dimensions.
    \item Compute $p$-values for substantively meaningful range of $\theta$.
	    Or calculate boundaries of regions.
  \end{enumerate}


  \note{So, let me just summarize our workflow --- which, for those of you
    familiar with Rosenbaum's approach to Fisher's hypothesis testing, will be
    very familiar.}

\end{frame}


\subsection{Information arises from both theory and instruments}

\begin{frame}
	\frametitle{Robust test statistics can increase power.}

	For the simple mean difference test statistic, we have $p=.375$, for a
	rank sum test (the sum of the ranks of the treated units), we have a
	$p=.4375$, and for an M-estimator based test (like mean-differences
	but with weights roughly inversely proportional to the
	Cook's $D$ influence measure) we have $p=.3125$.

	\centering
	\includegraphics[width=.8\textwidth]{newspapersCooksD.pdf}

\end{frame}

\begin{frame}
	\frametitle{Covariance adjusted tests can increase power.}

	Using the difference pre-vs-post as the outcome (comparing treated
	pre-vs-post with paired control pre-vs-post), and using the robust
	test statistic for $H_0: y_{i1}=y_{i0}$ $p=4/16=.25$.

	Using $e_i = (Y_i - Y_{i,t-1}) - (\hat{\beta}_0 + \hat{\beta}_1\text{pop} +
	\hat{\beta}_2 \text{num candidates})$ and the robust test statistic to
	test $H_0: y_{i1}=y_{i0}$  we have  $p=2/16=.125$.


\end{frame}

\end{document}



\begin{frame}[containsverbatim] %,allowframebreaks]
  \frametitle{Testing the Sharp Null of No Effects Exactly}

<<>>=
options(width=132)
@

<<sharpnulltest1exact,echo=TRUE,results='markup',highlight=TRUE,tidy=FALSE,tidy.opts=list(width.cutoff=130)>>=
Z <- c(0,1,0,1) ## Define treatment vector
Y <- c(16,22,7,14) ## Define outcome vector
## There are choose(4,2) ways to assign treatment
## Make a matrix containing all of the ways to assign treatment
Om <- matrix(0,ncol=choose(4,2),nrow=length(Z)) ## First fill with zeros
whotrted <- combn(1:4,2) ## Generate indicators of who is treated
for(i in 1:choose(4,2)){ Om[cbind(whotrted[,i],i)]<-1 }

## Great a function to summarize the hypothesized
## treatment to outcome relationship
meandifftz <- function(y,z){ mean(y[z==1]) - mean(y[z==0]) }
rankdifftz <- function(y,z){ q<-rank(y); mean(q[z==1]) - mean(q[z==0]) }
## Apply the function to all possible experiments
mdist<-apply(Om,2, function(z){ meandifftz(Y,z) })
rdist<-apply(Om,2, function(z){ rankdifftz(Y,z) })
rbind(Om,mdist,rdist)
@

\end{frame}

\begin{frame}[containsverbatim]
	\frametitle{continued}

<<sharpnulltest2,echo=TRUE,results='markup',highlight=TRUE,tidy=FALSE,tidy.opts=list(width.cutoff=130)>>=
table(mdist)
table(rdist)
mobs <- meandifftz(Y,Z) ## observed value of test stat
robs <- rankdifftz(Y,Z) ## observed value of test stat
mean(mdist >= mobs) ## p-value
mean(rdist >= robs) ## p-value
@

\end{frame}

\begin{frame}[containsverbatim,shrink]
	\frametitle{Faster Method}

<<installdevritools, eval=FALSE,echo=FALSE>>=
## Only run this once
if(!dir.exists("libraries")){ dir.create("libraries") }
.libPaths("libraries") ## make the default place for libraries the local one
installedpackages<-installed.packages()
if(!any(installedpackages[,"Package"]=="devtools")) {
  install.packages("devtools", repos="http://cran.rstudio.com")
  ## system("touch libraries/RItools/INSTALLED")
}
library(devtools)
install_github("markmfredrickson/RItools@randomization-distribution")
@

<<sharpnulltest3,echo=TRUE,results='markup',highlight=TRUE,tidy=FALSE,tidy.opts=list(width.cutoff=130)>>=
library(RItools,lib.loc="libraries") ## see fishersinference.Rnw to get the dev version

## Specify design: only 1 block
randomassignment<-simpleRandomSampler(z=Z,b=rep(1,4))

meandiffTZ <- function(ys, z) {
    mean(ys[!(!z)]) - mean(ys[!z])
}

meandifftest<-RItest(y=Y, z=Z, test.stat=meandiffTZ, sampler = randomassignment)
meandifftest
@

