---
title: How to ensure that policy experiments are credible and actionable
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: Jake Bowers
bibliography:
 - refs.bib
 - BIB/master.bib
 - BIB/misc.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: xelatex
    citation_package: biblatex
    template: icpsr.beamer
    incremental: true
    includes:
        in_header:
           - defs-all.sty
---


<!-- Make this document using library(rmarkdown); render("day12.Rmd") -->


```{r include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.
rm(list=ls())

require(knitr)

## This plus size="\\scriptsize" from https://stackoverflow.com/questions/26372138/beamer-presentation-rstudio-change-font-size-for-chunk

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before)
    return(options$size)
})

knit_hooks$set(plotdefault = function(before, options, envir) {
    if (before) par(mar = c(3, 3, .1, .1),oma=rep(0,4),mgp=c(1.5,.5,0))
})

opts_chunk$set(
  tidy=FALSE,     # display code as typed
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='\\scriptsize',
  out.width='.8\\textwidth',
  fig.retina=FALSE,
  message=FALSE,
  comment=NA,
  mysize=TRUE,
  plotdefault=TRUE)

if(!file.exists('figs')) dir.create('figs')

options(digits=4,
	scipen=8,
	width=132
	)
```

```{r eval=FALSE, include=FALSE, echo=FALSE}
## Run this only once and then not again until we want a new version from github
library('devtools')
library('withr')
with_libpaths('./libraries', install_github("markmfredrickson/RItools"), 'pre')
```

```{r echo=FALSE}
library(dplyr)
library(RItools,lib.loc="./libraries")
library(lmtest)
library(sandwich)
library(clubSandwich)
library(coin)
```

## Overall

\begin{enumerate}
\item 
\item 
\item 
\item 
\item 
\end{enumerate}


# A proposed research integrity process

## The Current DRAFT OES Process

 1.  **Project Initiation**, in which early ideas about the project are discussed to ensure general feasibility, proper planning, and wise investment of team resources prior to formally initiating a project and committing to a collaboration with agency partners
 2.  **Design Review**, in which the project design is peer reviewed and then presented to the team, to ensure a sound design that effectively addresses research objectives before we invest resources in fielding a study
 3. **Analysis Plan Commitment**, in which an analysis plan (also known as a “pre-analysis plan” or “pre-specification plan”) is finalized, date-stamped, and posted publicly on our website before data are received and analyzed
 4.  **Findings Review**, in which an initial analysis of results is presented to the team, to ensure that tentative findings are consistent with a sound analysis of the data, that important limitations on the study’s findings have been identified, and that alternative explanations have been addressed to the greatest extent possible
 5.  **Reanalysis**, an internal replication of the initial analysis, to ensure that results and conclusions are sound, reliable, and reproducible
 6. **Pre-Publication Review**,  to ensure OES maintains transparency, retains materials necessary for reproducibility, and meets all legal and administrative requirements in disseminating knowledge for the whole of government and the public


# Standard Operating Procedures

## Current DRAFT OES SOP Pieces

Overall, 

 1. Collaborate. Serve the agency partner. Practice humility and listening.
 2. Work in public as much as possible
 3. Randomization is a reasoned basis for statistical inference (i.e.  $p$-values should refer to distributions generated by design).

# Remember about approaches to statistical inference for causal quantities

## Approaches to creating interpretable comparisons:

   - Randomized experiments (more precision from reducing heterogeneity in $Y$)
   - Instrumental variables (with randomized $Z$ created $D$)
   - Natural Experiments / Discontinuities (one $X$ creates $Z$)
   - Difference-in-Differences (reduce bias *and* increase precision from reducing heterogeneity in $Y$)
   - Semi-/Non-parametric Covariance Adjustment (i.e. matching)
   - Parametric covariance adjustment

# What about interference? How define, estimate and test causal effects.

## Statistical inference for counterfactual quantities with interference?




# A Simple Design-based Estimation Approach


## Approaches for going beyond the sharp-null of no effects


**Estimation:**

   - Use design to isolate units
   - Or weight average differences by model of propagation / spillover \parencite{aronow2013estimating,toulis2013estimation}

**Testing:**

   - Assess implications of models of network-propagation effects \parencite{bowersetal2013}.
   - Invert hypothesis tests comparing levels/ranks of treatment outcomes to the uniformity trial \parencite{rosenbaum2007a}.


## Estimation restricting interference by design

Imagine that $Z_i \in \{U,C,T\}$ where  $T$ is treatment
(election observers), $C$ is control with possible spillover and and $U$ is
"uniformity trial" or control with no possible spillover. 

\medskip


Thus, if you have isolated units and randomization (such that *all units have
positive probability of $Z_i \in \{U,C,T\}$*) we have $y_{i,T}$, $y_{i,C}$, and
$y_{i,U}$ for each unit.^[The two-level design
\parencite{sinclair2012detecting}. See also \textcite[Chap 8]{gerbergreen2012}
or generalized saturation design \parencite{baird2014designing}.
\textcite{liu2014large} for some nice theory.]

\pause
\medskip

And you can define and estimate $\bar{\tau}_{\text{spillover}}=\bar{y}_{C} -
\bar{y}_{U}$ or $\bar{\tau}_{\text{Direct Effect}} = \bar{y}_{T} - \bar{y}_U$
etc..



## Estimation restricting interference by design

How would we use this data to estimate direct, indirect, or spillover effects?



## Summary

  - The sharp null implies no interference. So no need for an *assumption* of no interference.
  - Careful design can allow estimation when the number of potential outcomes is small-ish per unit.
  - Models of effects can specify flexible theoretical models of propagation over networks and randomization can justify statistical inference and causal inference.

## References

