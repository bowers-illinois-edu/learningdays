---
title: What is a hypothesis test? Why test hypotheses?
author: Jake Bowers and EGAP Learning Days Instructors
institute: University of Illinois @ Urbana-Champaign among other affiliations
bibliography: ../../Research-Group-Bibliography/big.bib
date: 9 April 2019 --- Bogotá
header-includes: \graphicspath{{.}{../images/}}
output:
  binb::metropolis:
    citation_package: natbib    
---

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}




```{r include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.
rm(list=ls())

require(knitr)

## This plus size="\\scriptsize" from https://stackoverflow.com/questions/26372138/beamer-presentation-rstudio-change-font-size-for-chunk

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before)
    return(options$size)
})

knit_hooks$set(plotdefault = function(before, options, envir) {
    if (before) par(mar = c(3, 3, .1, .1),oma=rep(0,4),mgp=c(1.5,.5,0))
})

opts_chunk$set(
  tidy=FALSE,     # display code as typed
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='\\scriptsize',
  out.width='.95\\textwidth',
  fig.retina=FALSE,
  message=FALSE,
  comment=NA,
  mysize=TRUE,
  plotdefault=TRUE)

if(!file.exists('figs')) dir.create('figs')

options(digits=4,
	scipen=8,
	width=132
	)
```

# Overview

## Key Points for this lecture

Statistical inference (e.g. Hypothesis tests and confidence
intervals) is \textbf{inference} --- reasoning about the unobserved.

$p$-values require probability distributions.

\textcolor{blue}{Randomization} (or Design) +
\textcolor{orange}{a Hypothesis} + \textcolor{green}{a Test Statistic
Function} can provide
probability distributions representing the hypothesis (and thus
$p$-values).

## Using randomization to reason about causal \textcolor{orange}{Inference}

How can we use what we \textbf{see} to learn about \only<1>{what we want to
\textbf{know}} \only<2->{\textbf{potential outcomes} ($\text{causal effect}_i=f(y_{i,1},y_{i,0})$}?


```{r readdata, echo=FALSE}
newsdf <- read.csv("../Data/news.csv")
```

```{r setupnewsdesigntab,echo=FALSE}
newsdf$y1 <- ifelse(newsdf$z==1,newsdf$r,"?")
newsdf$y0 <- ifelse(newsdf$z==0,newsdf$r,"?")

library(xtable)

newspapers.xtab<-xtable(newsdf[,c("city","s","z","rpre","r","Newspaper","y1","y0")],
			label="tab:newspapers",
			caption="Design and outcomes in the Newspapers Experiment.
			The Treatment column shows treatment randomized
			within pair with the
			newspaper ads as 1 and lack of treatment as 0. The
			potential outcomes are $y_{1}$ for treatment and
			$y_{0}$ for control.  \\citet{pana2006}
			provides more detail on the design of the experiment.",
			align=c("l","l","c","c","c","c","c","c","c"))
```

```{r newsdesigntab,results='asis',echo=FALSE}
print(newspapers.xtab,
      sanitize.text.function=function(x){x},include.rownames=FALSE,include.colnames=FALSE,
      table.placement="!ht",size="small",
      comment=FALSE,
      add.to.row=list(pos=list(-1),
		      command="&&& \\multicolumn{2}{c}{Turnout} \\\\ City &
		      Pair & Treat & \\multicolumn{1}{c}{Baseline} &
		      \\multicolumn{1}{c}{Outcome} &
		      \\multicolumn{1}{c}{Newspaper} &
		      \\multicolumn{1}{c}{$y_{1}$} &
		      \\multicolumn{1}{c}{$y_{0}$}\\\\"),
      hline.after=c(0,nrow(newspapers.xtab)))
```

\only<3>{\textbf{\emph{Small group exercise:}} \textbf{Can you think of
		\textcolor{orange}{more than one way} to learn about the
		counterfactual causal effect of treatment using what we
		observe from an experiment?}}


## What is the true effect of the treatment assignment?

  \only<1>{
    \includegraphics[width=.8\textwidth]{cartoonNeymanBayesFisherCropped.pdf} }
  \only<2>{ \includegraphics[width=.8\textwidth]{cartoon3ATENeyman.pdf} }
  \only<3>{ \includegraphics[width=.8\textwidth]{cartoonBayes.pdf} }
  \only<4>{ \includegraphics[width=.8\textwidth]{cartoon4Fisher.pdf} }


## Ingredients of a hypothesis test

 - A **hypothesis** is a statement about a relationship among potential outcomes (Strong or Weak)
 - A **test statistic** summarizes the relationship between treatment and
   observed outcomes.
 - The **design** allows us to link the hypothesis and the test statistic:
   calculate a test statistic that describes a relationship between potential
   outcomes.
 - The **design** also generates a distribution of possible test statistics
   implied by the hypothesis
 - A $p$-value describes the relationship between our observed test statistic
   and the possible hypothesized test statistics


```{r setup, echo=FALSE}
library(randomizr)
library(coin)
```

```{r echo=FALSE}
## First, create some data, 
##  y0 is potential outcome to control
N <- 10
y0 <- c(0,0,0,1,1,3,4,5,190,200)
## Different individual level treatment effects
tau <- c(10,30,200,90,10,20,30,40,90,20)
## y1 is potential outcome to treatment
y1 <- y0 + tau
#sd(y0)
#mean(y1)-mean(y0)
# mean(tau)
## Z is treatment assignment
set.seed(12345)
Z <- complete_ra(N)
## Y is observed outcomes
Y <- Z*y1 + (1-Z)*y0
## The data
dat <- data.frame(Y=Y,Z=Z,y0=y0,tau=tau,y1=y1)
dat$Ybin <- as.numeric(dat$Y > 100)
#dat
#pvalue(oneway_test(Y~factor(Z),data=dat,distribution=exact(),alternative="less"))
#pvalue(wilcox_test(Y~factor(Z),data=dat,distribution=exact(),alternative="less"))
```

## A hypothesis is a statement about or model of a relationship between potential outcomes

```{r}
kable(dat)
```

For example, the sharp, or strong, null hypothesis of no effects is $H_0: y_{i,1} = y_{i,0}$


## Test statistics summarize treatment to outcome relationships

```{r}
## The mean difference test statistic
meanTZ <- function(ys,z){ 
	mean(ys[z==1]) - mean(ys[z==0])
}

## The difference of mean ranks test statistic
meanrankTZ <- function(ys,z){
	ranky <- rank(ys)
	mean(ranky[z==1]) - mean(ranky[z==0])
}

observedMeanTZ <- meanTZ(ys=Y,z=Z)
observedMeanRankTZ <- meanrankTZ(ys=Y,z=Z)
observedMeanTZ
observedMeanRankTZ
```

## Linking test statistic and hypothesis.

What we observe for each person, $i$, ($Y_i$) is either what we would have
observed in treatment ($y_{i,1}$) **or** what we would have observed in
control ($y_{i,0}$).

$$Y_i = Z_i y_{i,1} + (1-Z_i)* y_{i,0}$$

So, if $y_{i,1}=y_{i,0}$ then:  $Y_i = y_{i,0}$: What we actually observe is
what we would have observed in the control condition.

## Generating the distribution of hypothetical test statistics

We need to know how to repeat our experiment:

```{r}
repeatExperiment <- function(N){
	complete_ra(N)
}
```

Then  we repeat it,  calculating the implied test statistic each time:

```{r reps, cache=TRUE}
set.seed(123456)
possibleMeanDiffsH0 <- replicate(10000,meanTZ(ys=Y,z=repeatExperiment(N=10)))
set.seed(123456)
possibleMeanRankDiffsH0 <- replicate(10000,meanrankTZ(ys=Y,z=repeatExperiment(N=10)))
```

## Plot the randomization distributions under the null

```{r fig.cap="An example of using the design of the experiment to test a hypothesis.", results='asis', echo=FALSE, fig.align='center'}
par(mfrow=c(1,2),mgp=c(1.5,.5,0),mar=c(3,3,0,0),oma=c(0,0,3,0))
plot(density(possibleMeanDiffsH0),
     ylim=c(0,.04),
     xlim=range(possibleMeanDiffsH0),
     lwd=2,
     main="",#Mean Difference Test Statistic",
     xlab="Mean Differences Consistent with H0")
rug(possibleMeanDiffsH0)
rug(observedMeanTZ,lwd=3,ticksize = .51)
text(observedMeanTZ-4,.022,"Observed Test Statistic")

plot(density(possibleMeanRankDiffsH0),lwd=2,
     ylim=c(0,.45),
     xlim=c(-10,10), #range(possibleMeanDiffsH0),
     main="", #Mean Difference of Ranks Test Statistic",
     xlab="Mean Difference of Ranks Consistent with H0")
rug(possibleMeanRankDiffsH0)
rug(observedMeanRankTZ,lwd=3,ticksize = .9)
text(observedMeanRankTZ,.45,"Observed Test Statistic")

mtext(side=3,outer=TRUE,text=expression(paste("Distributions of Test Statistics Consistent with the Design and ",H0: y[i1]==y[i0])))
```

## P-values summarize the plots

```{r calcpvalues}
pMeanTZ <- mean( possibleMeanDiffsH0 >= observedMeanTZ )
pMeanRankTZ <- mean( possibleMeanRankDiffsH0 >= observedMeanRankTZ )
pMeanTZ
pMeanRankTZ
```

## How to do this in R.

```{r}
## using the coin package
library(coin)
set.seed(12345)
pMean2 <- pvalue(oneway_test(Y~factor(Z),data=dat,distribution=approximate(B=1000)))
dat$rankY <- rank(dat$Y)
pMeanRank2 <- pvalue(oneway_test(rankY~factor(Z),data=dat,distribution=approximate(B=1000)))
pMean2
pMeanRank2
## using a development version of the RItools package 
library(devtools)
dev_mode()
install_github("markmfredrickson/RItools@randomization-distribution")
library(RItools)
thedesignA <- simpleRandomSampler(total=N,z=dat$Z,b=rep(1,N))
pMean4 <- RItest(y=dat$Y,z=dat$Z,samples=1000, test.stat= meanTZ ,
		 sampler = thedesignA)
pMeanRank4 <- RItest(y=dat$Y,z=dat$Z,samples=1000, test.stat= meanrankTZ ,
		     sampler = thedesignA)
pMean4
pMeanRank4
dev_mode()
```

## How to do this in R.

```{r}
## using the ri2 package
library(ri2)
thedesign <- declare_ra(N=N)
pMean4 <- conduct_ri( Y ~ Z, declaration = thedesign, 
		     sharp_hypothesis = 0, data = dat, sims = 1000)
summary(pMean4)
pMeanRank4 <- conduct_ri( rankY ~ Z, declaration = thedesign, 
		     sharp_hypothesis = 0, data = dat, sims = 1000)
summary(pMeanRank4)
```


## Next topics:

 - Testing weak null hypotheses $H_0: \bar{y}_{1} = \bar{y}_{0}$ 
 - Rejecting null hypotheses (and making false positive and/or false negative
   errors)
 - Power of hypothesis tests
 - Maintaining correct false positive error rates when testing more than one
   hypothesis. 



## Testing the weak null of no average effects

The weak null hypothesis is a claim about aggregates, and is nearly always
stated in terms of averages: $H_0: \bar{y}_{1} = \bar{y}_{0}$ The test
statistic for this hypothesis nearly always is the difference of means (i.e.
`meanTZ()` above.

```{r}
lm1 <- lm(Y~Z,data=dat)
lm1P <- summary(lm1)$coef["Z","Pr(>|t|)"]
ttestP1 <- t.test(Y~Z,data=dat)$p.value
library(estimatr)
ttestP2 <- difference_in_means(Y~Z,data=dat)
c(lm1P, ttestP1, ttestP2$p.value)
```

Why is the OLS $p$-value different? What assumptions is it making?

## Testing the weak null of no average effects

```{r results='asis'}
boxplot(Y~Z,data=dat)
```


## Testing the weak null of no average effects

```{r}
## By hand:
varEstATE <- function(Y,Z){
	var(Y[Z==1])/sum(Z) + var(Y[Z==0])/sum(1-Z)
}
seEstATE <- sqrt(varEstATE(dat$Y,dat$Z))
obsTStat <- observedMeanTZ/seEstATE
c(observedTestStat=observedMeanTZ,stderror=seEstATE,tstat=obsTStat,
  pval=2*min(pt(obsTStat,df=8,lower.tail = TRUE),
	     pt(obsTStat,df=8,lower.tail = FALSE))
  )
```

## Rejecting hypotheses and making errors

How should we interpret `r  pMeanTZ`? What about `r pMeanRankTZ`?

What does it mean to "reject" $H_0: y_{i,1}=y_{i,2}$ at $\alpha=.05$?

"In typical use, the level of the test [$\alpha$] is a promise about the
test’s performance and the size is a fact about its performance..." (Rosenbaum
2010, Glossary)

## Decision imply errors

If errors are necessary, how can we diagnose them? How to learn whether our
hypothesis testing procedure might generate too many false positive errors?

Diagnose by simulation:


## Diagnosing false positive rates by simulation

Across repetitions of the design:

 - Create a true null hypothesis.
 - Test the true null.
 - The $p$-value should be large.

The proportion of small $p$-values should be no larger than $\alpha$.


## Diagnosing false positive rates by simulation

Example with a binary outcome.

```{r}
collectPValues <- function(y,z){
	## Make Y and Z have no relationship by re-randomizing Z
	newz <- repeatExperiment(length(y))
        thelm <- lm(y~newz,data=dat)
	ttestP2 <- difference_in_means(y~newz,data=dat)
	owP <- pvalue(oneway_test(y~factor(newz),distribution=exact()))
	ranky <- rank(y)
	owRankP <- pvalue(oneway_test(ranky~factor(newz),distribution=exact()))
	return(c(lmp=summary(thelm)$coef["newz","Pr(>|t|)"],
		 neyp=ttestP2$p.value[[1]],
		 rtp=owP,
		 rtpRank=owRankP))
}
```

```{r fprdsim, cache=TRUE}
set.seed(12345)
pDist <- replicate(5000,collectPValues(y=dat$Ybin,z=dat$Z))
```

## Diagnosing false positive rates by simulation

```{r}
apply(pDist,1,table)
```

## Diagnosing false positive rates by simulation


```{r}
apply(pDist,1,function(x){ mean(x<.1)})
apply(pDist,1,function(x){ mean(x<.25)})
```


## Diagnosing false positive rates by simulation

```{r plotecdf, results='asis', echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,1),mgp=c(1.25,.5,0),oma=rep(0,4),mar=c(3,3,0,0))
plot(c(0,1),c(0,1),type="n",
     xlab="p-value=p",ylab="Proportion p-values < p")
for(i in 1:nrow(pDist)){
	lines(ecdf(pDist[i,]),pch=i,col=i)
}
abline(0,1,col="gray")
legend("topleft",legend=c("OLS","Neyman","Rand Inf Mean Diff","Rand Inf Mean Diff Ranks"),
			  pch=1:5,col=1:5,lty=1,bty="n")


```

## Topics for later

 - Power of tests

## Summary:

A good test (1) casts doubt on the truth rarely and (2)  easily
   distinguishes signal from noise (casts doubt on falsehoods often).

We can learn whether our testing procedure controls false positive rates given
our design.

When false positive rates are not controlled, what might be going wrong?
(often has to do with asymptotics)


## What else to know about hypothesis tests.

Here we list a few other important but advanced topics connected to hypothesis
testing:

 - Even if a given testing procedure controls the false positive rate for a
   single test, it may not control the rate for a group of multiple tests. See
   [10 Things you need to know about multiple
   comparisons](https://egap.org/methods-guides/10-things-you-need-know-about-multiple-comparisons)
   for a guide to the approaches to controlling such rejection-rates in
   multiple tests.
 - A $100\alpha$\% confidence interval can be defined as the range of
   hypotheses where all of the $p$-values are greater than or equal to
   $\alpha$. This is called inverting the hypothesis test.
   (@rosenbaum2010design). That is, a confidence interval is a collection of
   hypothesis tests.
 - A point estimate based on hypothesis testing is called a Hodges-Lehmann
   point estimate. (@rosenbaum1993hlp,@hodges1963elb)

## What else to know about hypothesis tests.

 - A set of hypothesis tests can be combined into one single hypothesis test
   (@hansen:bowers:2008,@caughey2017nonparametric)
 - In equivalence testing, one can hypothesize that two test-statistics are
   equivalent (i.e. the treatment group is the same as the control group)
   rather than only about one test-statistic (the difference between the two
   groups is zero) {@hartman2018equivalence}
 - Since a hypothesis test is a model of potential outcomes, one can use
   hypothesis testing to learn about complex models, such as models of
   spillover and propagation of treatment effects across networks
   (@bowers2013reasoning, @bowers2016research, @bowers2018models)


# References
