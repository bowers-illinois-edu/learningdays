\documentclass[10pt,ignorenonframetext]{beamer}


<<include=FALSE,cache=FALSE>>=
opts_chunk$set(tidy=FALSE,echo=FALSE,results='markup',fig.path='figs/fig',cache=FALSE,highlight=TRUE,width.cutoff=132,size='footnotesize',out.width='1.2\\textwidth',message=FALSE,comment=NA)
@

\usepackage{bowers-beamer}
\graphicspath{{.}{../images/}}
\newcommand{\framen}[1]{\frame{#1 \note{ }}}

\title[Average Treatment Effects]{Why use an Average Treatment Effect?\\
	How to do statistical inference for average treatmente effects?}

\author[Bowers]{
  Jake Bowers\inst{1}$,$\inst{2}
}

\date[9 May 2016]{EGAP Learning Days 4: Chile}

\institute[Illinois]{
  \inst{1}
  Political Science \& Statistics \& NCSA @ University of Illinois

  \smallskip

  \inst{2}
  White House Social and Behavioral Sciences Team

  \smallskip

  {jwbowers@illinois.edu   --- \href{http://jakebowers.org}{http://jakebowers.org} }
}

\renewcommand{\bibsection}{\subsection*{References}}


\begin{document}

\begin{frame}[plain,label=intro,noframenumbering]
  \titlepage
\end{frame}

\begin{frame}
	\frametitle{Why randomize?}
To facilitate interpretable statements about comparisons (i.e. to remove confounds, to decide
on intervention).

\bigskip

To facilitate interpretable statements about information (i.e. to justify
hypothesis tests and estimators.)

\end{frame}

\section{What does it mean to say that $E(\widehat{\text{ATE}})=\text{ATE}$}

\begin{frame}
	\frametitle{Recall the ATE}

	Like hypotheses and imputation, the ATE can help with the fundamental
	problem of causal inference.

	\centering
	\includegraphics[width=.8\textwidth]{cartoon3ATENeyman.pdf}

	What does the ATE ($=\bar{y}_1 - \bar{y}_0$ mean? Why report the $\widehat{\text{ATE}}$?
\end{frame}


<<readdata, echo=FALSE>>=
newsdf <- read.csv("../Data/news.csv")
@

<<expanddata, echo=FALSE>>=
newdats <- lapply(5:10,function(i){
	dat<-newsdf[,c("r","rpre")]+runif(nrow(newsdf),min=min(newsdf$rpre),max=max(newsdf$rpre))
	dat$s<-newsdf$s+i
	dat$z<-newsdf$z
	return(dat)
})

dat <- do.call("rbind",newdats)

@

\begin{frame}[containsverbatim]
	\frametitle{$E(\widehat{\text{ATE}})=\text{ATE}$ means that
		$\widehat{\text{ATE}}$ is unbiased for $\text{ATE}$. }


<<biassimsetup,echo=TRUE,results='markup',highlight=TRUE,tidy=FALSE,tidy.opts=list(width.cutoff=130)>>=
## Bias refers to a relationship between the repeated operation of a procedure and a truth. So we have to invent a truth.
dat$y0<-dat$rpre ## create true potential outcomes to control
trueATE<-.2 ## posit a true average treatment effect
dat$y1<-dat$y0+trueATE+rnorm(nrow(dat),mean=0,sd=sd(dat$y0)) ## create potential outcomes to treatment
dat$obsy<-with(dat, z*y1+(1-z)*y0 ) ## what we observe
trueATE<-with(dat,mean(y1)-mean(y0))
estATE<-coef(lm(obsy~z,dat))["z"] ## same as a mean difference on obsy
## Define two functions: (1) calc est ATE and (2) re-assign treatment
makeNewObsyAndEst<-function(thez){
    newobsy<-with(dat, thez*y1+(1-thez)*y0 )
    lmATE<-coef(lm(newobsy~thez))[["thez"]]
    return(c(lmATE=lmATE))
}
makeNewZ<-function(thez,theb){
	unsplit(lapply(split(thez,theb),sample),theb)
}
## Does the pair of functions do what we want them to do?
replicate(5,makeNewObsyAndEst(makeNewZ(dat$z,dat$s)))
@

\end{frame}

\begin{frame}[containsverbatim]

<<biassim,echo=TRUE,results='markup',highlight=TRUE,tidy=TRUE,tidy.opts=list(width.cutoff=130)>>=
nsims<-1000
set.seed(20150313)

## For many of the possible ways to run the experiment, calculate this mean difference
## The slow way: dist.sample.est<-replicate(nsims,makeNewObsyAndEst(makeNewObsyAndEst(makeNewZ(dat$z,dat$s)))
## The fast way uses all of the cores on your unix-based machine (mac or linux):
require(parallel)
ncores<-detectCores()
dist.sample.est<-simplify2array( mclapply(1:nsims,function(i){
						 makeNewObsyAndEst(makeNewZ(dat$z,dat$s))
                                 },mc.cores=ncores))

c(EestATE=mean(dist.sample.est),ATE=trueATE,estATE=estATE)

## And recall that we have simulation error on the order of 1/sqrt(nsims)
SEsims<-sqrt(var(dist.sample.est)/nsims)
SEsims
@

\end{frame}

\begin{frame}[fragile]
	\frametitle{What does it mean to say we have an unbiased estimator?}

	\centering
<<biasplot, fig.width=6,fig.height=6,out.width='.6\\linewidth',echo=FALSE>>=
par(oma=rep(0,4),mgp=c(1.5,.5,0),mar=c(5,3,0,0),xpd=NA)
plot(density(dist.sample.est),main="",xlab="Est. Average Treatment Effects")
rug(dist.sample.est)
rug(trueATE,ticksize=.2,lwd=2)
rug(estATE,ticksize=.1,lwd=2)
rug(mean(dist.sample.est),ticksize=.1,lwd=2)
@

\end{frame}


\section{Confidence Intervals}

\begin{frame}
	\frametitle{Confidence Interval Ingredients}

	$$ \text{CI}(\text{ATE})=\widehat{\text{ATE}} \pm z_{\alpha/2}
	\text{SE}(\widehat{\text{ATE}} ) $$

	where, $z_{\alpha/2}$ for $\alpha=.05$ is 1.96.

\end{frame}


<<echo=FALSE>>=
confint.HC<-function (object, parm, level = 0.95, thevcov, ...) {
  ## a copy of the confint.lm function adding "thevcov" argument
  cf <- coef(object)
  pnames <- names(cf)
  if (missing(parm))
    parm <- pnames
  else if (is.numeric(parm))
    parm <- pnames[parm]
  a <- (1 - level)/2
  a <- c(a, 1 - a)
  fac <- qt(a, object$df.residual)
  pct <- stats:::format.perc(a, 3)
  ci <- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,
							     pct))
  ## The original version extracts  the var-cov matrix itself
  ## ses <- sqrt(diag(vcov(object)))[parm]
  ses <- sqrt(diag(thevcov))[parm]
  ci[] <- cf[parm] + ses %o% fac
  ci
}
@


\begin{frame}[containsverbatim,shrink]
	\frametitle{Standard Errors for the Estimated ATE}

	What is a standard error in the context of a randomized experiment?

	Here, pretending that the randomization was simple and not blocked.

<<trueSE,echo=TRUE,results='markup',highlight=TRUE,tidy=TRUE,tidy.opts=list(width.cutoff=130)>>=
## See the Dunning / Freedman, Pisani, Purves derivation
y0<-dat$y0
y1<-dat$y1
Z<-dat$z
Y<-Z*y1+(1-Z)*y0
V<-var(cbind(y0,y1))
varc<-V[1,1]
vart<-V[2,2]
covtc<-V[1,2]
N<-length(y0)
n<-sum(Z)
m<-N-n
varestATE<-((N-n)/(N-1))*(vart/n) + ((N-m)/(N-1))* (varc/m) + (2/(N-1)) * covtc
## And the *feasible* version (where we do not observe the potential outcomes)
varYc<-var(Y[Z==0])
varYt<-var(Y[Z==1])
fvarestATE<-(N/(N-1)) * ( (varYt/n) + (varYc/m) )
@

\end{frame}

\begin{frame}[containsverbatim]


<<compareSEs,echo=TRUE,results='markup',highlight=TRUE,tidy=TRUE,tidy.opts=list(width.cutoff=130)>>=

lm1<-lm(Y~Z)
iidSElm1<- sqrt(diag(vcov(lm1)))[["Z"]]

c(trueSE=sqrt(varestATE),feasible=sqrt(fvarestATE),iid=iidSElm1, simSE=sd(dist.sample.est))

@

\end{frame}

\begin{frame}[containsverbatim]
	\frametitle{Different Confidence Intervals}


<<CIs,echo=TRUE,results='markup',highlight=TRUE,tidy=TRUE,tidy.opts=list(width.cutoff=130)>>=
theiidci<-confint(lm1,level=.95,parm="Z")
feasCI<-estATE + c(1,-1) * qnorm(.05/2) * sqrt(fvarestATE)
bestCI<-estATE + c(1,-1) * qnorm(.05/2) * sqrt(varestATE)

rbind(feasCI,
      theiidci,
      bestCI
      )

@

\end{frame}

\begin{frame}[containsverbatim]
	\frametitle{Which is better?}
	
	A good test casts doubt on the truth rarely.
	
	\medskip

	A good confidence interval contains the truth at least $100\alpha$ \%
	of the time. (Because a confidence interval is a collection of
	hypotheses against which we have little information to argue. A
	confidence interval is collection of unsurprising hypotheses.)

\end{frame}

\begin{frame}[containsverbatim]

	\frametitle{Checking Coverage}

<<checkCIs,cache=TRUE,echo=TRUE,results='markup',highlight=TRUE,tidy=TRUE,tidy.opts=list(width.cutoff=130)>>=
makeFeasibleSE<-function(y,z){
	varYc<-var(y[z==0])
	varYt<-var(y[z==1])
	N <- length(y)
	stopifnot(N==length(z)) ## a test of the code
	fvarestATE<-(N/(N-1)) * ( (varYt/n) + (varYc/m) )
	return(fvarestATE)
}

makeCIs<-function(y,thez){
	lm1<-lm(y~thez)
	estATE<-coef(lm1)["thez"]
	theiidci<-confint(lm1,level=.95,parm="thez")
	fvarestATE <- makeFeasibleSE(y=y,z=thez)
	thefeasci<-estATE + c(1,-1) * qnorm(.05/2) * sqrt(fvarestATE)
	truthinIIDci <- 0 >= min(theiidci) & 0 <= max(theiidci)
	truthinFeasci <- 0 >= min(thefeasci) & 0 <= max(thefeasci)
	return(c(truthinIIDci=truthinIIDci,
		 truthinFeasci=truthinFeasci))
}

makeCIs(y=Y,thez=sample(Z))
@

\end{frame}

\begin{frame}[containsverbatim]
	\frametitle{Checking Coverage}

<<checkCIsres,cache=TRUE,echo=TRUE,results='markup',highlight=TRUE,tidy=TRUE,tidy.opts=list(width.cutoff=130)>>=
set.seed(20160509)
nsims <- 10000
coverageCheck<-simplify2array( mclapply(1:nsims,function(i){
						 makeCIs(y=Y,thez=sample(Z))
                                 },mc.cores=ncores))
##coverageCheck<-replicate(10000, makeCIs(y=Y,thez=sample(Z))) ##makeNewZ(Z,Y)))
apply(coverageCheck,1,mean)
@

\end{frame}


\begin{frame}
	\frametitle{Review}

	What is unbiasedness? Why do we care? How would we assess bias?

	\medskip

	What is a confidence interval? How would we assess coverage?
\end{frame}

\begin{frame}[allowframebreaks]
        \bibliographystyle{apsr}
        \bibliography{../BIB/big}
\end{frame}

\end{document}

